{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from autoencoder import TabularDataset, Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/TrainingWiDS2021.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = []\n",
    "for field in data.columns:\n",
    "    if data[field].dtype == 'object':\n",
    "        categorical.append(field)\n",
    "    try:\n",
    "        if data[field].describe()['max'] == data[field].describe()['min']:\n",
    "            print('*****************************************************************')\n",
    "            print(field)\n",
    "    except:\n",
    "        pass\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readmission_status has value 0, drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns='readmission_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3) \n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_memory_usage_of_data_frame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(data, columns=categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_memory_usage_of_data_frame(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intriguing. One hot encoding saved memory. I guess that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['diabetes_mellitus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot.drop(columns=['diabetes_mellitus'])\n",
    "one_hot = one_hot.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = list(set(one_hot.columns)-set(categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(one_hot, procs=[Normalize], cont_names=continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now make a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(to.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otto = Autoencoder(to.xs.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(otto.parameters(), lr=LEARNING_RATE)\n",
    "otto.float()\n",
    "otto.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for batch in dls:     \n",
    "        # clear the optimizer of previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward ---------------------------------------------\n",
    "        output= otto(batch[1]) #batch is a list of three vectors: \n",
    "                # [y, cont_X, and cat_X]. We already converted categorical to continuous, so pass in batch[1]\n",
    "        loss = criterion(output, batch[1])\n",
    "        # backward --------------------------------------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # keep a running total of loss for the batches in this epoch\n",
    "        running_loss += loss.item()\n",
    "    # log\n",
    "    lossb = running_loss / len(dls)\n",
    "    train_loss.append(lossb)\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, NUM_EPOCHS, loss.data), lossb)\n",
    "    \n",
    "otto.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './autoencoder_state.txt'\n",
    "torch.save(otto.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(NUM_EPOCHS)], train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
